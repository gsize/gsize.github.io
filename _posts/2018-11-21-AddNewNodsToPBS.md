---
layout: post
title: PBS集群系统中增加计算节点
description: 在原有的PBS集群系统中增加计算节点
category: blog
---

# 集群系统中增加计算节点 #


公司在2015年搭建了一套ubuntu14.04系统为基础的集群系统，包括一个主节点和4个计算节点。
计算任务由torque软件负责调度。
运行到现在有3年多。随着计算任务的增多，需要扩充这套系统的计算能力，增加了两个计算节点，每个节点24个物理核。

原计划在给新增的节点服务器重新安装与原有系统一样系统。但发现重装的系统会存在软件库版本与集群的有差异。
因此，在同事的建议下，采用clonezilla（再生龙）工具把集群中的一个节点的系统制作成一个镜像，在把它灌装到新增的计算节点上。
并在这个系统的基础上修改少量的配置（/etc/hosts和/etc/network/interface），使得新增节点顺利并入集群系统。

在主节点中修改torque的server_priv/nodes配置，添加新增节点信息。
接着用qterm -t quick关闭torque系统，重启主节点的pbs_server程序，通过pbsnodes命令查看所有节点是否正常工作。

期间，卡在一个奇怪现象。集群主节点与子节点的数据交换由万兆光纤网卡和光交换机完成。
新增的网卡是intel i350型号，在ubuntu14.04系统中有eth2、eth3两个设备名对应。
我指定eth2为静态ip，eth3动态获取。在网络上显示这两个网口处于同一个网段，这两个ip都能远程访问，
但我只接了其中一个网口，但分配的两个ip都能ping得通。
结果是pbs系统只连接那个动态分配的ip，导致pbsnods显示对应节点状态为down.

猜测计算节点的网口和interface的网口配置不对应。
修改interface的eth2为eth3,，重启网卡，消除了这个怪问题。


[Gsize]:    http://gsize.github.io  "Gsize"
